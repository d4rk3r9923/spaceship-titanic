{"cells":[{"cell_type":"markdown","metadata":{"id":"GBPnP47mO-RQ"},"source":["# Spaceship Titanic"]},{"cell_type":"markdown","metadata":{"id":"9RFxAFLMO7DP"},"source":["https://www.kaggle.com/competitions/spaceship-titanic"]},{"cell_type":"markdown","metadata":{"id":"0GLZni4xwsRB"},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"cellView":"code","id":"gscOavpffIBP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686184578187,"user_tz":-420,"elapsed":102497,"user":{"displayName":"Tráº§n Huy","userId":"11815461795022219161"}},"outputId":"42b5bd64-8cab-4c86-96c4-0c54e8544794"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","%matplotlib inline\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, RobustScaler\n","from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, precision_score, recall_score, f1_score, auc\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from xgboost import XGBClassifier\n","\n","import warnings\n","warnings.simplefilter(\"ignore\")\n","\n","from google.colab import drive  # Mount Google Drive if you are using Google Colab\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"ZziaPo8Sc8Tf"},"source":["# Download from Kaggle and Load Spaceship Titanic Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qn3cx-lYhwkA"},"outputs":[],"source":["# Create '.kaggle' folder in 'root'\n","!mkdir ~/.kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MbsQJ5HVyEIU"},"outputs":[],"source":["# Copy 'kaggle.json' to recently created folder\n","!cp '/content/drive/MyDrive/Colab Notebooks/DPL302m/spaceship-titanic/kaggle.json' ~/.kaggle/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6OJSwBKkyCo0"},"outputs":[],"source":["# Set permission\n","!chmod 600 ~/.kaggle/kaggle.json "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dvORFxjsx_Nb"},"outputs":[],"source":["# Download dataset from kaggle competition\n","!kaggle competitions download -c spaceship-titanic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTMw8yJahXQN"},"outputs":[],"source":["# Unzip files\n","!unzip spaceship-titanic.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6CCsdLtPkBEc"},"outputs":[],"source":["# Run this if you wish to remove the zip\n","!rm spaceship-titanic.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rTQnqfnMkJFH"},"outputs":[],"source":["# Load train and test\n","train = pd.read_csv(\"/content/train.csv\")\n","test = pd.read_csv(\"/content/test.csv\")"]},{"cell_type":"markdown","metadata":{"id":"QbvVo2VW1sfa"},"source":["# Data Exploration and Preprocessing, Feature Engineering"]},{"cell_type":"markdown","metadata":{"id":"bAcyhHWUOuNn"},"source":[">The task is to predict whether a passenger was transported to an alternate dimension during the Spaceship Titanic's collision with the spacetime anomaly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VNgExuXTRoSl"},"outputs":[],"source":["train.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHptCbYb1nZD"},"outputs":[],"source":["test.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"omE-o2SiRvv2"},"outputs":[],"source":["print(\"Train shape:\", train.shape)\n","print(\"Test shape:\", test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YXR9GxH4pOQM"},"outputs":[],"source":["numerical = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Cabin_num']\n","categorical = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side', 'FirstName', 'LastName', 'Group']\n","encoded_categorical = ['Encoded_HomePlanet', 'Encoded_CryoSleep', 'Encoded_Destination', 'Encoded_VIP', 'Encoded_Deck', 'Encoded_Side', 'Encoded_FirstName', 'Encoded_LastName', 'Encoded_Group']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ezxbVFnBRwNs"},"outputs":[],"source":["def preprocess(data):\n","  # Display missing data before preprocessing\n","  print(\"Missing Data Before Preprocessing: {}\".format(data.isnull().sum().sum()))\n","  print(data.isnull().sum())\n","  print()\n","\n","  # Split columns\n","  data[[\"Deck\", \"Cabin_num\", \"Side\"]] = data[\"Cabin\"].str.split(\"/\", expand=True)\n","  data[[\"ID\", \"Group\"]] = data[\"PassengerId\"].str.split(\"_\", expand=True)\n","  data[[\"FirstName\", \"LastName\"]] = data[\"Name\"].str.split(\" \", expand=True)\n","\n","  # Convert to float data type\n","  data['Cabin_num'] = data['Cabin_num'].astype(float)\n","\n","  # Drop the original columns\n","  data.drop(['Cabin', 'PassengerId', 'ID', 'Name'], axis=1, inplace=True)\n","\n","  # Handling missing values for both numerical and categorical features\n","  for feature in numerical + categorical:\n","    if feature in numerical:\n","      # Replace missing values with the median of the respective feature\n","      data[feature].replace(np.nan, data[feature].median(), inplace=True)\n","    else:\n","      # Fill missing values with the mode (most frequent value) of the respective feature\n","      data[feature].fillna(data[feature].mode().values[0], inplace=True)\n","\n","  # Drop duplicate records\n","  data.drop_duplicates(inplace=True)\n","\n","  # Convert all numerical features to integers\n","  data[numerical] = data[numerical].astype(int)\n","\n","  # Display missing data after preprocessing\n","  print(\"Missing Data After Preprocessing: {}\".format(data.isnull().sum().sum()))\n","  print(data.isnull().sum())\n","  print()\n","\n","  # Display duplicate records after preprocessing\n","  print(\"Duplicate Records After Preprocessing: {}\".format(data.duplicated().sum()))\n","  print()\n","\n","  # Return the preprocessed DataFrame without outliers\n","  return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i9LKwh1DlZSN"},"outputs":[],"source":["df_train = preprocess(train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ce5aopPblfib"},"outputs":[],"source":["df_test = preprocess(test)"]},{"cell_type":"markdown","metadata":{"id":"6yRB0snbOxpK"},"source":["# Exploratory Data Analysis (EDA)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27EvYuQ7VdgQ"},"outputs":[],"source":["df_train.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3wQfhl5oMuE"},"outputs":[],"source":["print('Unique Indexes:', df_train.index.is_unique)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xX3b361gR4dj"},"outputs":[],"source":["df_train.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jSqjmQgAS0i7"},"outputs":[],"source":["df_train.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crq_BAzZS10D"},"outputs":[],"source":["df_train.describe(include = 'object')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IoafjI4Kwpa0"},"outputs":[],"source":["df_train.describe(include = 'bool')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2BdBMF2ceGO"},"outputs":[],"source":["df_train['Transported'].value_counts().plot(kind=\"bar\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tqthU3HlWnOZ"},"outputs":[],"source":["df_train.corr()"]},{"cell_type":"markdown","metadata":{"id":"QrkBFRjEO2on"},"source":["# Feature Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GpVgxaJZjvmu"},"outputs":[],"source":["# Initialize the LabelEncoder\n","label_encoder = LabelEncoder()\n","\n","# Perform label encoding on each categorical feature in the 'categorical' list\n","for count, i in enumerate(categorical):\n","  df_train[i] = label_encoder.fit_transform(df_train[i])\n","  encoded_labels = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n","  print(f\"Encoded Labels for {i}: {encoded_labels}\")\n","\n","# Create new columns with encoded values for each categorical feature\n","for column, new_column in zip(categorical, encoded_categorical):\n","  df_train[new_column] = label_encoder.fit_transform(df_train[column])\n","  encoded_labels = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n","\n","# Drop the original categorical columns\n","df_train.drop(columns=categorical, inplace=True)\n","\n","# Inspect the train data\n","df_train.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STUBYqRi4I8k"},"outputs":[],"source":["# Initialize the LabelEncoder\n","label_encoder = LabelEncoder()\n","\n","# Perform label encoding on each categorical feature in the 'categorical' list\n","encoded_labels = {}\n","for count, i in enumerate(categorical):\n","    df_test[i] = label_encoder.fit_transform(df_test[i])\n","    encoded_labels = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n","    print(f\"Encoded Labels for {i}: {encoded_labels}\")\n","\n","# Create new columns with encoded values for each categorical feature\n","for column, new_column in zip(categorical, encoded_categorical):\n","    df_test[new_column] = label_encoder.fit_transform(df_test[column])\n","    encoded_labels[new_column] = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n","\n","# Drop the original categorical columns\n","df_test.drop(columns=categorical, inplace=True)\n","\n","# Inspect the test data\n","df_test.info()"]},{"cell_type":"markdown","metadata":{"id":"4BSPHicMQGsa"},"source":["# Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OrYPQd4FmwpO"},"outputs":[],"source":["# Splitting the data into features (x) and target variable (y)\n","x = df_train.drop(\"Transported\", axis=1)  # Features (input variables)\n","y = df_train[\"Transported\"]  # Target variable (output variable)\n","\n","# Performing the train-test split\n","x_train_raw, x_test_raw, y_train_raw, y_test_raw = train_test_split(x, y, test_size=0.3, random_state=42, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"at5js08yh8PA"},"outputs":[],"source":["# Define the feature scaler\n","scaler = RobustScaler()\n","\n","# Define the pipeline for feature scaling\n","pipeline = Pipeline([\n","    ('scaler', scaler),\n","    ('model', None)\n","])\n","\n","# Scale the features using the pipeline\n","x_train = pipeline.fit_transform(x_train_raw)\n","x_test = pipeline.transform(x_test_raw)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GnWfB9AX0MBq"},"outputs":[],"source":["# Encode the target variable\n","y_train = label_encoder.fit_transform(y_train_raw)\n","y_test = label_encoder.transform(y_test_raw)\n","\n","# Print the encoded classes\n","print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oIa9ZzqEhpOV"},"outputs":[],"source":["# Define the models\n","models = [\n","    ('Logistic Regression', LogisticRegression()),\n","    ('KNN', KNeighborsClassifier()),\n","    ('SVM', SVC()),\n","    ('Decision Tree', DecisionTreeClassifier()),\n","    ('Random Forest', RandomForestClassifier()),\n","    ('Naive Bayes', GaussianNB()),\n","    ('XGBoost', XGBClassifier())\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3S25iJdPhsQh"},"outputs":[],"source":["# Define the hyperparameters to tune for each model\n","param_grid = {\n","    'Logistic Regression': {\n","        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n","        'penalty': ['l1', 'l2'],\n","        'solver': ['liblinear', 'saga']\n","    },\n","    'KNN': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance']\n","    },\n","    'SVM': {\n","        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n","        'kernel': ['linear', 'rbf']\n","    },\n","    'Decision Tree': {\n","        'max_depth': [None, 5, 10, 15],\n","        'min_samples_split': [2, 5, 10]\n","    },\n","    'Random Forest': {\n","        'n_estimators': [50, 100, 200],\n","        'max_depth': [None, 5, 10],\n","        'min_samples_split': [2, 5, 10]\n","    },\n","    'Naive Bayes': {},  # No hyperparameters for Gaussian Naive Bayes\n","    'XGBoost': {\n","        'n_estimators': [100, 200, 300],\n","        'learning_rate': [0.01, 0.1, 0.2],\n","        'max_depth': [3, 5, 7],\n","        'subsample': [0.8, 1.0],\n","        'colsample_bytree': [0.8, 1.0],\n","        'gamma': [0, 1, 5],\n","        'reg_alpha': [0, 0.1, 0.5],\n","        'reg_lambda': [0, 0.1, 0.5]\n","    }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2JggpUuhtPj"},"outputs":[],"source":["# Perform GridSearchCV for each model\n","best_models = {}\n","for model_name, model in models:\n","    pipeline.set_params(model=model)\n","    grid_search = GridSearchCV(pipeline.named_steps['model'], param_grid[model_name], cv=5)\n","    grid_search.fit(x_train, y_train)\n","    best_models[model_name] = grid_search.best_estimator_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"taqmGv4V5jWJ"},"outputs":[],"source":["# Define lists to store the evaluation results\n","model_names = []\n","accuracy_scores = []\n","recall_scores = []\n","precision_scores = []\n","f1_scores = []\n","auc_roc_scores = []\n","confusion_matrices = []\n","roc_curves = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbA7Ain35l6y"},"outputs":[],"source":["# Evaluate each model\n","for model_name, model in best_models.items():\n","    # Fit the model on the training data\n","    model.fit(x_train, y_train)\n","    \n","    # Predict the target variable for the test data\n","    y_pred = model.predict(x_test)\n","    \n","    # Calculate evaluation metrics\n","    accuracy = accuracy_score(y_test, y_pred)\n","    recall = recall_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","    auc_roc = roc_auc_score(y_test, y_pred)\n","    \n","    # Calculate the confusion matrix\n","    cm = confusion_matrix(y_test, y_pred)\n","    \n","    # Calculate the ROC curve\n","    fpr, tpr, _ = roc_curve(y_test, y_pred)\n","    \n","    # Append the results to the lists\n","    model_names.append(model_name)\n","    accuracy_scores.append(accuracy)\n","    recall_scores.append(recall)\n","    precision_scores.append(precision)\n","    f1_scores.append(f1)\n","    auc_roc_scores.append(auc_roc)\n","    confusion_matrices.append(cm)\n","    roc_curves.append((fpr, tpr))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ww3rEExw5nsZ"},"outputs":[],"source":["# Create a DataFrame of evaluation results\n","results_df = pd.DataFrame({\n","    'Model': model_names,\n","    'Accuracy': accuracy_scores,\n","    'Recall': recall_scores,\n","    'Precision': precision_scores,\n","    'F1 Score': f1_scores,\n","    'AUC-ROC': auc_roc_scores\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wzIblrVc5pFd"},"outputs":[],"source":["# Print the DataFrame\n","results_df.head(6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFTnnG8Z5xVY"},"outputs":[],"source":["# Plot the confusion matrices\n","for model_name, cm in zip(model_names, confusion_matrices):\n","    plt.figure()\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","    plt.title(f'Confusion Matrix - {model_name}')\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VYi64D0O5v2v"},"outputs":[],"source":["# Create a single plot for all ROC curves\n","plt.figure()\n","\n","# Plot the ROC curves\n","for model_name, (fpr, tpr) in zip(model_names, roc_curves):\n","    plt.plot(fpr, tpr, label=model_name)\n","\n","# Plot the random guess line\n","plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n","\n","# Set the plot title and labels\n","plt.title('ROC Curves - All Models')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","\n","# Add a legend\n","plt.legend()\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hoWAiKMV00pG"},"outputs":[],"source":["# Find the best model based on a chosen metric\n","best_model_idx = results_df['Accuracy'].idxmax()\n","best_model = best_models[model_names[best_model_idx]]\n","print('Best Model:')\n","print(best_model)"]},{"cell_type":"markdown","source":["# Deep Neural Network"],"metadata":{"id":"bUOSoLoMQTYo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"l6Wo3DKq1HNn"},"outputs":[],"source":["early_stopping1 = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 10, restore_best_weights = True) \n","early_stopping2 = tf.keras.callbacks.EarlyStopping(monitor = \"val_accuracy\", patience = 10, restore_best_weights = True) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kglix0Rj1Prb"},"outputs":[],"source":["model = tf.keras.Sequential([\n","        tf.keras.layers.Input(name = \"input\", shape = (x_train.shape[1])),\n","        tf.keras.layers.Dense(256, activation = \"relu\"),\n","        tf.keras.layers.BatchNormalization(),\n","        tf.keras.layers.Dropout(0.2),\n","        tf.keras.layers.Dense(128, activation = \"relu\"),\n","        tf.keras.layers.BatchNormalization(),\n","        tf.keras.layers.Dropout(0.2),\n","        tf.keras.layers.Dense(64, activation = \"relu\"),\n","        tf.keras.layers.Dense(max(y_train)+1, activation = \"softmax\")])\n","        \n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t5MT7aMU1ot7"},"outputs":[],"source":["model.compile(optimizer = tf.keras.optimizers.Adam(),\n","              loss = \"sparse_categorical_crossentropy\",\n","              metrics = [\"accuracy\"])\n"," \n","model_history = model.fit(x_train, y_train,\n","                epochs = 100,\n","                verbose = 1, batch_size = 128,\n","                validation_data = (x_test, y_test),\n","                callbacks = [early_stopping1, early_stopping2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5q_3yD_9178r"},"outputs":[],"source":["print(model.evaluate(x_train, y_train)) \n","print(model.evaluate(x_test, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZ9Mh8tS1-ny"},"outputs":[],"source":["plt.plot(model_history.history[\"loss\"]) \n","plt.plot(model_history.history[\"val_loss\"]) \n","plt.legend([\"loss\", \"validation loss\"], loc =\"upper right\")\n","plt.title(\"Train and Validation Loss\") \n","plt.xlabel(\"epoch\") \n","plt.ylabel(\"Sparse Categorical Cross Entropy\") \n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJApkNGl2BRj"},"outputs":[],"source":["plt.plot(model_history.history[\"accuracy\"])\n","plt.plot(model_history.history[\"val_accuracy\"])\n","plt.legend([\"accuracy\", \"validation accuracy\"], loc =\"upper right\")\n","plt.title(\"Train and Validation Accuracy\") \n","plt.xlabel(\"epoch\") \n","plt.ylabel(\"Accuracy\") \n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G3eeedYs2Mmw"},"outputs":[],"source":["df_test['Transported'] = np.empty(df_test.shape[0])\n","df_pred = df_test.drop('Transported', axis = 1).iloc[0:]\n","df_pred\n","y_pred = model.predict(scaler.transform(df_pred)).argmax(axis=1)\n","print('Prediction in Numerical ', y_pred)\n","print('Prediction in Text ', label_encoder.inverse_transform(y_pred))\n","# print('Actual Value ', df_test.iloc[0:]['genre'].to_numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WFtM5XdZ80ti"},"outputs":[],"source":["np.unique(y_pred, return_counts=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BMq3N1kRvkkC"},"outputs":[],"source":["model.save(\"saved.h5\")\n","loaded_model = keras.models.load_model(\"saved.h5\")"]}],"metadata":{"colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyN9AbJgDwHW5fJJbuyeFIoR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}